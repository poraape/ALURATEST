{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q google-generativeai google-ai-generativelanguage==0.6.15 gradio langchain llama-index pandas openai anthropic langchain_google_genai langchain_community google-api-python-client"
      ],
      "metadata": {
        "id": "5UsO1tnUJizd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 2: Importa√ß√µes Iniciais, Configura√ß√£o de Logging e Configura√ß√µes da Aplica√ß√£o\n",
        "import os\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "# from google.colab import userdata # Descomente se estiver no Colab e usando secrets\n",
        "\n",
        "# Configura√ß√£o do Logger\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configura√ß√µes da Aplica√ß√£o (Externalizadas)\n",
        "APP_CONFIG = {\n",
        "    \"gemini_model\": {\n",
        "        \"model_id\": \"gemini-2.0-flash\",\n",
        "        \"generation_config\": {\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 40,\n",
        "            \"max_output_tokens\": 8192,\n",
        "        },\n",
        "        \"safety_settings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        ],\n",
        "        \"request_timeout\": 120, # Timeout em segundos para a chamada da API (AUMENTADO PARA 120s)\n",
        "    },\n",
        "    \"rag\": {\n",
        "        \"embeddings_model_name\": \"models/embedding-001\",\n",
        "        \"retriever_k\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Bloco 2: Logging e configura√ß√µes da aplica√ß√£o (com timeout) definidos.\")"
      ],
      "metadata": {
        "id": "HlIOpUn1Jk7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 3: Configura√ß√£o da API Key do Google e Cliente Generative AI\n",
        "try:\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "    # from google.colab import userdata\n",
        "    # GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "        logger.info(\"GOOGLE_API_KEY carregada com sucesso.\")\n",
        "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        logger.info(\"Cliente Google Generative AI configurado.\")\n",
        "    else:\n",
        "        logger.critical(\"Erro Cr√≠tico: A secret 'GOOGLE_API_KEY' ou vari√°vel de ambiente n√£o foi encontrada.\")\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY not found. A execu√ß√£o n√£o pode continuar sem a API Key.\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Ocorreu um erro inesperado ao configurar a API Key ou o cliente GenAI: {e}\")\n",
        "    raise\n",
        "print(\"Bloco 3: Configura√ß√£o da API Key e cliente GenAI processada.\")"
      ],
      "metadata": {
        "id": "uHOh02gbJ26q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 4: Configura√ß√£o do Modelo Gemini e Fun√ß√£o Auxiliar invocar_gemini\n",
        "try:\n",
        "    model_config = APP_CONFIG[\"gemini_model\"]\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_config[\"model_id\"],\n",
        "        generation_config=model_config[\"generation_config\"],\n",
        "        safety_settings=model_config[\"safety_settings\"]\n",
        "    )\n",
        "    logger.info(f\"Modelo Gemini '{model_config['model_id']}' inicializado com sucesso.\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Erro ao inicializar o modelo Gemini '{model_config['model_id']}': {e}\")\n",
        "    raise\n",
        "\n",
        "def invocar_gemini(prompt_usuario: str, contexto_sistema: str = \"Voc√™ √© um assistente de pesquisa cient√≠fica altamente qualificado.\") -> str:\n",
        "    full_prompt = (\n",
        "        f\"{contexto_sistema}\\n\\n\"\n",
        "        \"--- IN√çCIO DA TAREFA DO USU√ÅRIO ---\\n\"\n",
        "        f\"{prompt_usuario}\\n\"\n",
        "        \"--- FIM DA TAREFA DO USU√ÅRIO ---\"\n",
        "    )\n",
        "    logger.info(f\"Invocando Gemini. Contexto: '{contexto_sistema[:50]}...', Prompt (primeiros 100 chars): {prompt_usuario[:100]}...\")\n",
        "\n",
        "    request_options = {\"timeout\": APP_CONFIG[\"gemini_model\"].get(\"request_timeout\", 60)} # Default 60s, configurado para 120s\n",
        "\n",
        "    try:\n",
        "        logger.debug(f\"Enviando para Gemini com timeout de {request_options['timeout']}s. Prompt completo (primeiros 200 chars): {full_prompt[:200]}\")\n",
        "        response = model.generate_content(full_prompt, request_options=request_options)\n",
        "        logger.info(\"Resposta recebida do Gemini.\")\n",
        "\n",
        "        if response.parts:\n",
        "            generated_text = \"\".join(part.text for part in response.parts if hasattr(part, 'text'))\n",
        "            logger.debug(f\"Texto gerado (das partes): {generated_text[:100]}...\")\n",
        "            return generated_text\n",
        "        elif hasattr(response, 'text') and response.text:\n",
        "             generated_text = response.text\n",
        "             logger.debug(f\"Texto gerado (do atributo text): {generated_text[:100]}...\")\n",
        "             return generated_text\n",
        "        else:\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                block_reason_message = getattr(response.prompt_feedback, 'block_reason_message', str(response.prompt_feedback.block_reason))\n",
        "                logger.warning(f\"Conte√∫do bloqueado pelo Gemini. Raz√£o: {block_reason_message}\")\n",
        "                return f\"Conte√∫do bloqueado pela API. Raz√£o: {block_reason_message}\"\n",
        "            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n",
        "                 generated_text = \"\".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))\n",
        "                 logger.debug(f\"Texto gerado (dos candidatos): {generated_text[:100]}...\")\n",
        "                 return generated_text\n",
        "            logger.warning(\"Resposta do Gemini vazia ou em formato inesperado. Feedback: %s\", response.prompt_feedback)\n",
        "            return \"Resposta vazia ou em formato inesperado do modelo. Verifique os logs e a configura√ß√£o de seguran√ßa.\"\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Erro cr√≠tico ao interagir com o modelo Gemini: {e}\")\n",
        "        return f\"Erro cr√≠tico ao contatar o modelo Gemini: {str(e)}. Verifique os logs do servidor.\"\n",
        "\n",
        "print(\"Bloco 4: Modelo Gemini e fun√ß√£o invocar_gemini (com timeout e logging) definidos.\")"
      ],
      "metadata": {
        "id": "iTH4PV5OJ7H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 5: Defini√ß√£o da Classe Base para M√≥dulos (ModuloSynapseBase)\n",
        "class ModuloSynapseBase:\n",
        "    def __init__(self, nome_modulo: str, contexto_sistema_especifico: str, instrucao_principal: str):\n",
        "        self.nome_modulo = nome_modulo\n",
        "        self.contexto_sistema_completo = (\n",
        "            f\"Voc√™ √© o {self.nome_modulo} do Synapse Intellect. \"\n",
        "            f\"{contexto_sistema_especifico} \"\n",
        "            \"Responda de forma estruturada, seguindo as diretrizes fornecidas. \"\n",
        "            \"A entrada do usu√°rio estar√° claramente delimitada.\"\n",
        "        )\n",
        "        self.instrucao_principal = instrucao_principal\n",
        "\n",
        "    def construir_prompt_usuario_final(self, entrada_usuario: str) -> str:\n",
        "        return (\n",
        "            f\"{self.instrucao_principal}\\n\\n\"\n",
        "            f\"ENTRADA DO USU√ÅRIO PARA AN√ÅLISE:\\n'''\\n{entrada_usuario}\\n'''\\n\\n\"\n",
        "            \"POR FAVOR, PROCEDA COM A AN√ÅLISE CONFORME AS DIRETRIZES DO M√ìDULO.\"\n",
        "        )\n",
        "\n",
        "    def executar(self, entrada_usuario: str) -> str:\n",
        "        if not entrada_usuario or not entrada_usuario.strip():\n",
        "            logger.warning(f\"Entrada vazia recebida para o {self.nome_modulo}.\")\n",
        "            return \"Por favor, forne√ßa uma entrada v√°lida para o m√≥dulo.\"\n",
        "        prompt_usuario_final = self.construir_prompt_usuario_final(entrada_usuario)\n",
        "        logger.info(f\"Executando {self.nome_modulo} com entrada (primeiros 100 chars): {entrada_usuario[:100]}...\")\n",
        "        return invocar_gemini(prompt_usuario_final, self.contexto_sistema_completo)\n",
        "\n",
        "print(\"Bloco 5: Classe Base ModuloSynapseBase definida.\")"
      ],
      "metadata": {
        "id": "dB0tr4hHJ-eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 6: Implementa√ß√£o dos M√≥dulos Funcionais\n",
        "\n",
        "class ModuloInteligenciaEstrategica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Intelig√™ncia Estrat√©gica\",\n",
        "            contexto_sistema_especifico=\"Sua especialidade √© identificar tend√™ncias, lacunas e oportunidades em pesquisa.\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise o seguinte tema ou problema de pesquisa fornecido.\\nSua an√°lise deve incluir:\\n\"\n",
        "                \"1. Tend√™ncias emergentes e consolidadas.\\n2. Lacunas de conhecimento.\\n\"\n",
        "                \"3. Poss√≠veis impactos futuros.\\n4. Palavras-chave relevantes.\\nSeja prospectivo e estrat√©gico.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_a_inteligencia_estrategica(e: str): return ModuloInteligenciaEstrategica().executar(e)\n",
        "\n",
        "class ModuloIdeacaoValidacao(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Idea√ß√£o e Valida√ß√£o\",\n",
        "            contexto_sistema_especifico=\"Seu foco √© gerar e refinar ideias de pesquisa, avaliando originalidade e conex√µes.\",\n",
        "            instrucao_principal=(\n",
        "                \"Trabalhe com a seguinte ideia ou problema de pesquisa:\\nSua tarefa √©:\\n\"\n",
        "                \"1. Gerar 3-5 novas perguntas de pesquisa.\\n2. Avaliar originalidade.\\n\"\n",
        "                \"3. Sugerir 2-3 conex√µes interdisciplinares.\\n4. Apontar desafios.\\nFomente criatividade e rigor.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_b_ideacao_e_validacao(e: str): return ModuloIdeacaoValidacao().executar(e)\n",
        "\n",
        "class ModuloArquiteturaMetodologica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Arquitetura Metodol√≥gica\",\n",
        "            contexto_sistema_especifico=\"Sua especialidade √© desenhar planos de pesquisa robustos.\",\n",
        "            instrucao_principal=(\n",
        "                \"Desenvolva um esbo√ßo de plano metodol√≥gico para:\\nSeu esbo√ßo deve considerar:\\n\"\n",
        "                \"1. Objetivos SMART.\\n2. Abordagem metodol√≥gica.\\n3. Amostragem/corpus.\\n\"\n",
        "                \"4. Fases da pesquisa.\\n5. Instrumentos/t√©cnicas.\\nEstrutura clara e l√≥gica.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_c_arquitetura_metodologica(e: str): return ModuloArquiteturaMetodologica().executar(e)\n",
        "\n",
        "class ModuloRevisaoCriticaEtica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Revis√£o Cr√≠tica & √âtica\",\n",
        "            contexto_sistema_especifico=\"Atue como revisor experiente: rigor, viabilidade, √©tica.\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise a seguinte proposta/ideia/rascunho:\\nSua an√°lise cr√≠tica deve abordar:\\n\"\n",
        "                \"1. Clareza do problema/objetivos.\\n2. Viabilidade.\\n3. Potencial de impacto.\\n\"\n",
        "                \"4. Poss√≠veis vieses e mitiga√ß√£o.\\n5. Considera√ß√µes √©ticas.\\nSeja construtivo e rigoroso.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_d_revisao_critica_e_etica(e: str): return ModuloRevisaoCriticaEtica().executar(e)\n",
        "\n",
        "class ModuloEscritaCientifica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Escrita Cient√≠fica\",\n",
        "            contexto_sistema_especifico=\"Ajude a aprimorar textos acad√™micos: clareza, estrutura, rigor.\",\n",
        "            instrucao_principal=(\n",
        "                \"Trabalhe com o seguinte texto/solicita√ß√£o:\\nSua tarefa pode ser (adapte):\\n\"\n",
        "                \"1. Rascunho de se√ß√£o: melhorias.\\n2. Ideia para par√°grafo/se√ß√£o: desenvolvimento.\\n\"\n",
        "                \"3. D√∫vida sobre estilo/normas: orienta√ß√µes gerais.\\n4. Pedido de esbo√ßo: estrutura t√≠pica.\\nPriorize objetividade, precis√£o, concis√£o.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_e_escrita_cientifica(e: str): return ModuloEscritaCientifica().executar(e)\n",
        "\n",
        "class ModuloImpactoOperacoes(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Impacto & Opera√ß√µes Cient√≠ficas\",\n",
        "            contexto_sistema_especifico=\"Oriente sobre estrat√©gias de publica√ß√£o, visibilidade e gest√£o.\",\n",
        "            instrucao_principal=(\n",
        "                \"Forne√ßa orienta√ß√µes estrat√©gicas para:\\nSuas orienta√ß√µes devem cobrir:\\n\"\n",
        "                \"1. Sugest√µes de peri√≥dicos/confer√™ncias.\\n2. Estrat√©gias para visibilidade/impacto.\\n\"\n",
        "                \"3. KPIs para monitorar progresso.\\n4. Dicas para submiss√£o.\\nConselhos pr√°ticos.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_f_impacto_e_operacoes_cientificas(e: str): return ModuloImpactoOperacoes().executar(e)\n",
        "\n",
        "class ModuloCientometriaAplicada(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"M√≥dulo de Cientometria Aplicada\",\n",
        "            contexto_sistema_especifico=\"Explique e ajude a interpretar m√©tricas cient√≠ficas (conhecimento geral).\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise a seguinte consulta/dados sobre m√©tricas:\\nSua resposta deve:\\n\"\n",
        "                \"1. Pergunta geral: explique conceito, c√°lculo, uso, limita√ß√µes.\\n\"\n",
        "                \"2. Autor/artigo/revista: m√©tricas relevantes e interpreta√ß√£o.\\n\"\n",
        "                \"3. Dados num√©ricos: interpreta√ß√£o contextualizada.\\nSem acesso a bases de dados em tempo real.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_g_cientometria_aplicada(e: str): return ModuloCientometriaAplicada().executar(e)\n",
        "\n",
        "print(\"Bloco 6: M√≥dulos funcionais (todos usando Classe Base) definidos.\")"
      ],
      "metadata": {
        "id": "Lmp7jLRZKBJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 7: Mapeamento dos M√≥dulos para a Interface Gradio\n",
        "modulos_disponiveis = {\n",
        "    \"A. Intelig√™ncia Estrat√©gica\": modulo_a_inteligencia_estrategica,\n",
        "    \"B. Idea√ß√£o e Valida√ß√£o\": modulo_b_ideacao_e_validacao,\n",
        "    \"C. Arquitetura Metodol√≥gica\": modulo_c_arquitetura_metodologica,\n",
        "    \"D. Revis√£o Cr√≠tica & √âtica\": modulo_d_revisao_critica_e_etica,\n",
        "    \"E. Escrita Cient√≠fica\": modulo_e_escrita_cientifica,\n",
        "    \"F. Impacto & Opera√ß√µes Cient√≠ficas\": modulo_f_impacto_e_operacoes_cientificas,\n",
        "    \"G. Cientometria Aplicada\": modulo_g_cientometria_aplicada,\n",
        "}\n",
        "nomes_modulos_dropdown = list(modulos_disponiveis.keys())\n",
        "\n",
        "logger.info(\"Mapeamento de m√≥dulos para Gradio criado.\")\n",
        "print(\"Bloco 7: Mapeamento de m√≥dulos para Gradio criado.\")"
      ],
      "metadata": {
        "id": "EvIgWTcLKE03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 8: Esbo√ßo Conceitual para Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) - Comentado\n",
        "# Esta se√ß√£o √© um placeholder e requer implementa√ß√£o com seus pr√≥prios dados e configura√ß√£o detalhada.\n",
        "# As bibliotecas LangChain e LlamaIndex foram instaladas no Bloco 1.\n",
        "\n",
        "# --- IN√çCIO DO C√ìDIGO RAG DE EXEMPLO (DESCOMENTE E ADAPTE PARA TESTAR) ---\n",
        "\n",
        "# --- Prepara√ß√£o de Dados de Exemplo (VOC√ä PRECISA CRIAR ESTES ARQUIVOS) ---\n",
        "# Exemplo: Crie 'doc_exemplo_1.txt' e 'doc_exemplo_2.txt' no seu ambiente.\n",
        "\n",
        "# rag_retriever = None\n",
        "# rag_chain = None\n",
        "\n",
        "# def configurar_rag_simples():\n",
        "#     global rag_retriever, rag_chain\n",
        "#     logger.info(\"Tentando configurar o RAG simples...\")\n",
        "#     try:\n",
        "#         # Verifique se os arquivos de exemplo existem\n",
        "#         if not (os.path.exists(\"doc_exemplo_1.txt\") and os.path.exists(\"doc_exemplo_2.txt\")):\n",
        "#             logger.warning(\"Arquivos de exemplo para RAG (doc_exemplo_1.txt, doc_exemplo_2.txt) n√£o encontrados. RAG n√£o ser√° configurado.\")\n",
        "#             return\n",
        "\n",
        "#         from langchain_community.document_loaders import TextLoader\n",
        "#         from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#         from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "#         from langchain_community.vectorstores import FAISS\n",
        "#         from langchain.chains import RetrievalQA\n",
        "#         from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#         loader1 = TextLoader('doc_exemplo_1.txt', encoding='utf-8')\n",
        "#         loader2 = TextLoader('doc_exemplo_2.txt', encoding='utf-8')\n",
        "#         documents = loader1.load() + loader2.load()\n",
        "\n",
        "#         if not documents:\n",
        "#             logger.warning(\"Nenhum documento carregado para o RAG.\")\n",
        "#             return\n",
        "\n",
        "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "#         docs_split = text_splitter.split_documents(documents)\n",
        "\n",
        "#         if not docs_split:\n",
        "#             logger.warning(\"Nenhum documento dividido para o RAG.\")\n",
        "#             return\n",
        "\n",
        "#         embeddings = GoogleGenerativeAIEmbeddings(model=APP_CONFIG[\"rag\"][\"embeddings_model_name\"], task_type=\"retrieval_document\")\n",
        "#         vector_store = FAISS.from_documents(docs_split, embeddings)\n",
        "#         rag_retriever = vector_store.as_retriever(search_kwargs={\"k\": APP_CONFIG[\"rag\"][\"retriever_k\"]})\n",
        "\n",
        "#         llm_rag = ChatGoogleGenerativeAI(model=APP_CONFIG[\"gemini_model\"][\"model_id\"],\n",
        "#                                          temperature=0.5,\n",
        "#                                          convert_system_message_to_human=True)\n",
        "\n",
        "#         rag_chain = RetrievalQA.from_chain_type(\n",
        "#             llm=llm_rag,\n",
        "#             chain_type=\"stuff\",\n",
        "#             retriever=rag_retriever,\n",
        "#             return_source_documents=True\n",
        "#         )\n",
        "#         logger.info(\"Chain RetrievalQA (RAG) configurado com sucesso usando documentos de exemplo.\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.exception(f\"Erro ao configurar o exemplo RAG: {e}\")\n",
        "#         rag_chain = None # Garante que n√£o ser√° usado se falhar\n",
        "\n",
        "# def consultar_rag_com_gemini(query: str) -> str:\n",
        "#     if rag_chain is None:\n",
        "#         logger.warning(\"Sistema RAG (chain RetrievalQA) n√£o configurado. Chamada ignorada.\")\n",
        "#         return \"Sistema RAG n√£o est√° configurado. Verifique a c√©lula de configura√ß√£o RAG e os arquivos de exemplo.\"\n",
        "#     try:\n",
        "#         logger.info(f\"Consultando RAG com query: {query[:100]}...\")\n",
        "#         response = rag_chain.invoke({\"query\": query})\n",
        "#         answer = response.get(\"result\", \"N√£o foi poss√≠vel obter uma resposta do chain RAG.\")\n",
        "#         # source_documents = response.get(\"source_documents\", [])\n",
        "#         # if source_documents:\n",
        "#         #     answer += \"\\n\\n--- Documentos Fonte Recuperados (RAG) ---\"\n",
        "#         #     for i, doc in enumerate(source_documents):\n",
        "#         #         answer += f\"\\nFonte {i+1}: {doc.page_content[:100]}...\"\n",
        "#         return answer\n",
        "#     except Exception as e:\n",
        "#         logger.exception(f\"Erro durante a consulta RAG: {e}\")\n",
        "#         return f\"Erro durante a consulta RAG: {str(e)}\"\n",
        "\n",
        "# Descomente a linha abaixo para tentar configurar o RAG ao iniciar (requer arquivos de exemplo)\n",
        "# configurar_rag_simples()\n",
        "\n",
        "# --- FIM DO C√ìDIGO RAG DE EXEMPLO ---\n",
        "\n",
        "logger.info(\"Esbo√ßo conceitual RAG definido (atualmente comentado).\")\n",
        "print(\"Bloco 8: Esbo√ßo RAG (comentado) definido.\")"
      ],
      "metadata": {
        "id": "QCYyDAEOKHjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 9: Fun√ß√£o da Interface Gradio (interface_synapse_intellect)\n",
        "def interface_synapse_intellect(reflexao_inicial, modulo_selecionado, texto_para_modulo, usar_rag_checkbox):\n",
        "    # Log inicial da requisi√ß√£o\n",
        "    log_prefix = \"Interface Gradio:\"\n",
        "    logger.info(f\"{log_prefix} M√≥dulo: '{modulo_selecionado}', RAG: {usar_rag_checkbox}\")\n",
        "    logger.debug(f\"{log_prefix} Reflex√£o: {str(reflexao_inicial)[:100]}...\")\n",
        "    logger.debug(f\"{log_prefix} Entrada: {str(texto_para_modulo)[:100]}...\")\n",
        "\n",
        "    if not modulo_selecionado:\n",
        "        logger.warning(f\"{log_prefix} Nenhum m√≥dulo selecionado.\")\n",
        "        return \"Por favor, selecione um m√≥dulo.\", \"Nenhum m√≥dulo foi selecionado.\"\n",
        "    if not texto_para_modulo or not str(texto_para_modulo).strip():\n",
        "        logger.warning(f\"{log_prefix} Entrada vazia para '{modulo_selecionado}'.\")\n",
        "        return \"Por favor, forne√ßa a entrada espec√≠fica para o m√≥dulo.\", f\"A entrada para '{modulo_selecionado}' est√° vazia.\"\n",
        "\n",
        "    entrada_completa_para_modulo = str(texto_para_modulo)\n",
        "    if reflexao_inicial and str(reflexao_inicial).strip():\n",
        "        entrada_completa_para_modulo = (\n",
        "            f\"Contexto/Reflex√£o Inicial do Pesquisador:\\n'''\\n{str(reflexao_inicial)}\\n'''\\n\\n\"\n",
        "            f\"Tarefa Espec√≠fica para o M√≥dulo '{modulo_selecionado}':\\n'''\\n{str(texto_para_modulo)}\\n'''\"\n",
        "        )\n",
        "        logger.info(f\"{log_prefix} Reflex√£o inicial combinada com a entrada do m√≥dulo.\")\n",
        "\n",
        "    resultado_modulo = \"Processando...\" # Placeholder inicial\n",
        "    debrief_base_text = f\"Synapse Intellect (M√≥dulo: {modulo_selecionado}) processou sua solicita√ß√£o.\"\n",
        "\n",
        "    # A funcionalidade RAG permanece como esbo√ßo e n√£o ser√° ativada.\n",
        "    if usar_rag_checkbox:\n",
        "        logger.warning(f\"{log_prefix} RAG solicitado, mas n√£o est√° ativo/implementado.\")\n",
        "        debrief_base_text += \" (RAG solicitado mas n√£o ativo).\"\n",
        "        # Poderia adicionar um aviso ao resultado_modulo aqui se quisesse.\n",
        "\n",
        "    funcao_modulo = modulos_disponiveis.get(modulo_selecionado)\n",
        "    if funcao_modulo:\n",
        "        logger.info(f\"{log_prefix} Chamando fun√ß√£o para o m√≥dulo '{modulo_selecionado}'.\")\n",
        "        resultado_modulo = funcao_modulo(entrada_completa_para_modulo) # Chamada direta\n",
        "        logger.info(f\"{log_prefix} Resultado recebido do m√≥dulo '{modulo_selecionado}'.\")\n",
        "        debrief_base_text += \" (Resposta gerada pelo modelo Gemini).\"\n",
        "    else:\n",
        "        logger.error(f\"{log_prefix} M√≥dulo '{modulo_selecionado}' n√£o encontrado no mapeamento.\")\n",
        "        resultado_modulo = f\"Erro: M√≥dulo '{modulo_selecionado}' n√£o encontrado.\"\n",
        "        debrief_base_text = \"Erro cr√≠tico: M√≥dulo n√£o implementado.\"\n",
        "\n",
        "    # Debrief gerado pelo LLM\n",
        "    prompt_debrief_llm = (\n",
        "        f\"O usu√°rio interagiu com o '{modulo_selecionado}'. \"\n",
        "        f\"Descreva brevemente (1-2 senten√ßas) a estrat√©gia ou foco principal deste m√≥dulo. N√£o resuma a resposta ao usu√°rio.\"\n",
        "    )\n",
        "    contexto_debrief_llm = \"Voc√™ √© um assistente IA que descreve a fun√ß√£o de outros m√≥dulos IA de forma concisa.\"\n",
        "    debrief_gerado_pelo_llm = invocar_gemini(prompt_debrief_llm, contexto_debrief_llm)\n",
        "\n",
        "    final_debrief_text = f\"{debrief_base_text}\\n\\nEstrat√©gia do M√≥dulo (sugerida pela IA): {debrief_gerado_pelo_llm}\"\n",
        "    return resultado_modulo, final_debrief_text\n",
        "\n",
        "print(\"Bloco 9: Fun√ß√£o da interface Gradio definida.\")"
      ],
      "metadata": {
        "id": "yTba7DsiKK10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 10: Cria√ß√£o da Interface Gradio (com CSS Customizado)\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body, .gradio-container {\n",
        "    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;\n",
        "    background-color: #f0f2f5; /* Um cinza muito claro, estilo Apple */\n",
        "    color: #1d1d1f; /* Texto principal escuro */\n",
        "}\n",
        ".gr-panel { /* Usado para agrupar colunas/componentes */\n",
        "    background-color: #ffffff !important;\n",
        "    border: 1px solid #d2d2d7 !important; /* Borda sutil */\n",
        "    border-radius: 12px !important;\n",
        "    box-shadow: 0 1px 3px rgba(0,0,0,0.04), 0 1px 2px rgba(0,0,0,0.08) !important; /* Sombra mais sutil */\n",
        "    padding: 24px !important;\n",
        "}\n",
        ".gr-button {\n",
        "    background-color: #007aff !important; /* Azul Apple */\n",
        "    color: white !important;\n",
        "    border-radius: 8px !important;\n",
        "    font-weight: 500 !important;\n",
        "    border: none !important;\n",
        "    padding: 10px 18px !important;\n",
        "    box-shadow: 0 1px 2px rgba(0,0,0,0.05) !important;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    background-color: #0056b3 !important; /* Azul mais escuro no hover */\n",
        "}\n",
        ".gr-input input, .gr-textbox textarea, .gr-dropdown select, .gr-checkboxgroup input {\n",
        "    background-color: #f8f8fa !important; /* Fundo do campo de entrada um pouco mais claro */\n",
        "    border-radius: 8px !important;\n",
        "    border: 1px solid #c6c6c8 !important; /* Borda mais sutil */\n",
        "    color: #1d1d1f !important;\n",
        "    padding: 10px !important;\n",
        "}\n",
        ".gr-dropdown select { padding-right: 30px !important; } /* Espa√ßo para a seta */\n",
        "\n",
        ".gr-markdown h1 { font-size: 28px; font-weight: 600; color: #1d1d1f; margin-bottom: 12px; }\n",
        ".gr-markdown h3 { font-size: 18px; font-weight: 600; color: #1d1d1f; margin-top: 20px; margin-bottom: 8px; }\n",
        ".gr-markdown p, .gr-markdown li { color: #333333; line-height: 1.6; }\n",
        "\n",
        "label span, .gr-checkbox label span { /* Texto dos labels */\n",
        "    color: #555 !important;\n",
        "    font-weight: 500 !important;\n",
        "    font-size: 14px !important;\n",
        "}\n",
        ".gr-info { /* Texto de informa√ß√£o abaixo dos componentes */\n",
        "    color: #6e6e73 !important;\n",
        "    font-size: 12px !important;\n",
        "}\n",
        "#resultado_modulo_md { /* Sa√≠da principal Markdown */\n",
        "    background-color: #f8f9fa;\n",
        "    padding: 18px;\n",
        "    border-radius: 8px;\n",
        "    border: 1px solid #e9ecef;\n",
        "    min-height: 250px; /* Altura m√≠nima para a √°rea de resultado */\n",
        "}\n",
        "#output_debrief_modulo textarea { /* Textbox do Debrief */\n",
        "    background-color: #e9ecef !important; /* Fundo levemente diferente para o debrief */\n",
        "    font-size: 13px !important;\n",
        "    line-height: 1.5 !important;\n",
        "}\n",
        "/* Remover borda padr√£o do Gradio container se o tema Soft adicionar uma */\n",
        ".gradio-container { border: none !important; box-shadow: none !important; }\n",
        "\"\"\"\n",
        "try:\n",
        "    with gr.Blocks(theme=gr.themes.Default(text_size=gr.themes.sizes.text_md), css=custom_css, title=\"Synapse Intellect‚Ñ¢ v1.3\") as app_synapse: # Mudei para Default para ter mais controle com CSS\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # üß† Synapse Intellect‚Ñ¢\n",
        "            ### v1.3 - Seu Copiloto de IA para Pesquisa Cient√≠fica\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row(variant=\"panel\"): # Usando variant=\"panel\" para aplicar o estilo .gr-panel\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Fase 0: Reflex√£o Inicial (Opcional)\")\n",
        "                input_reflexao = gr.Textbox(\n",
        "                    label=\"Descreva seu problema de pesquisa ou ideia geral aqui\",\n",
        "                    lines=3, # Reduzido para interface mais compacta\n",
        "                    placeholder=\"Ex: Dificuldades na ades√£o ao tratamento...\"\n",
        "                )\n",
        "                gr.Markdown(\"### ‚öôÔ∏è M√≥dulo e Entrada\")\n",
        "                input_modulo_selecionado = gr.Dropdown(\n",
        "                    choices=nomes_modulos_dropdown,\n",
        "                    label=\"Selecione o M√≥dulo Funcional\",\n",
        "                    info=\"Escolha a fun√ß√£o que deseja utilizar.\"\n",
        "                )\n",
        "                input_texto_para_modulo = gr.Textbox(\n",
        "                    label=\"Entrada Espec√≠fica para o M√≥dulo\",\n",
        "                    lines=6, # Reduzido\n",
        "                    placeholder=\"Forne√ßa detalhes, perguntas, temas...\"\n",
        "                )\n",
        "                input_usar_rag = gr.Checkbox(\n",
        "                    label=\"Usar RAG (Funcionalidade em Esbo√ßo - N√ÉO ATIVA)\",\n",
        "                    value=False,\n",
        "                    interactive=False # Mantido desabilitado\n",
        "                )\n",
        "                botao_processar = gr.Button(\"üöÄ Processar com Synapse Intellect\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üí° Resultados Gerados\")\n",
        "                output_resultado_modulo = gr.Markdown(elem_id=\"resultado_modulo_md\", label=\"Resultado do M√≥dulo\")\n",
        "                output_debrief_modulo = gr.Textbox(\n",
        "                    elem_id=\"output_debrief_modulo\",\n",
        "                    label=\"Synapse Strategy Debrief‚Ñ¢\",\n",
        "                    lines=6, # Reduzido\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ---\n",
        "            <p style=\"text-align:center; font-size:12px; color:#888;\">\n",
        "            Prot√≥tipo v1.3. Valide criticamente as respostas. RAG n√£o ativo.\n",
        "            </p>\n",
        "            \"\"\"\n",
        "        )\n",
        "    logger.info(\"Interface Gradio v1.3 definida com CSS customizado.\")\n",
        "    print(\"Bloco 10: Interface Gradio v1.3 definida.\")\n",
        "except Exception as e:\n",
        "    logger.exception(\"Erro ao definir a interface Gradio v1.3.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "7etH8iTGKNfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 11: Lan√ßamento da Interface Gradio (Execu√ß√£o Principal)\n",
        "\n",
        "# Certifique-se que 'app_synapse' foi definido no Bloco 10\n",
        "if 'app_synapse' in locals():\n",
        "    logger.info(\"Iniciando a interface Gradio do Synapse Intellect‚Ñ¢ v1.3...\")\n",
        "    # A linha abaixo √© a que efetivamente inicia o Gradio em um notebook\n",
        "    app_synapse.launch(share=True, debug=True, inline=False)\n",
        "    logger.info(\"Interface Gradio iniciada. Verifique o link p√∫blico acima.\")\n",
        "else:\n",
        "    logger.error(\"O objeto 'app_synapse' (interface Gradio) n√£o foi definido. Verifique erros nos blocos anteriores, especialmente o Bloco 10.\")\n",
        "\n",
        "print(\"Bloco 11: Tentativa de lan√ßamento da interface Gradio conclu√≠da.\")\n",
        "print(\"Se a interface iniciou, voc√™ ver√° um link '.gradio.live' na sa√≠da acima desta mensagem.\")\n",
        "print(\"Se n√£o houver link ou houver erros, revise as sa√≠das das c√©lulas anteriores e os logs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fW--nmuKQVA",
        "outputId": "05d889e8-8fee-4b6c-c6ab-6420d0cf44cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aec6e9436b66f065de.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Erro cr√≠tico ao interagir com o modelo Gemini: HTTPConnectionPool(host='localhost', port=41941): Read timed out. (read timeout=120.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-31-d33493dd05d3>\", line 27, in invocar_gemini\n",
            "    response = model.generate_content(full_prompt, request_options=request_options)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 868, in generate_content\n",
            "    # - It may require specifying regional endpoints when creating the service\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1336, in __call__\n",
            "    response, generative_service.GenerateContentResponse\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1236, in _get_response\n",
            "    Args:\n",
            "          \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 537, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=41941): Read timed out. (read timeout=120.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uLEM4N9KUgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}