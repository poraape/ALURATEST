{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %pip install -q google-generativeai google-ai-generativelanguage==0.6.15 gradio langchain llama-index pandas openai anthropic langchain_google_genai langchain_community google-api-python-client"
      ],
      "metadata": {
        "id": "5UsO1tnUJizd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 2: Importações Iniciais, Configuração de Logging e Configurações da Aplicação\n",
        "import os\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "# from google.colab import userdata # Descomente se estiver no Colab e usando secrets\n",
        "\n",
        "# Configuração do Logger\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(module)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configurações da Aplicação (Externalizadas)\n",
        "APP_CONFIG = {\n",
        "    \"gemini_model\": {\n",
        "        \"model_id\": \"gemini-2.0-flash\",\n",
        "        \"generation_config\": {\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 40,\n",
        "            \"max_output_tokens\": 8192,\n",
        "        },\n",
        "        \"safety_settings\": [\n",
        "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "        ],\n",
        "        \"request_timeout\": 120, # Timeout em segundos para a chamada da API (AUMENTADO PARA 120s)\n",
        "    },\n",
        "    \"rag\": {\n",
        "        \"embeddings_model_name\": \"models/embedding-001\",\n",
        "        \"retriever_k\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Bloco 2: Logging e configurações da aplicação (com timeout) definidos.\")"
      ],
      "metadata": {
        "id": "HlIOpUn1Jk7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 3: Configuração da API Key do Google e Cliente Generative AI\n",
        "try:\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "    # from google.colab import userdata\n",
        "    # GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "        logger.info(\"GOOGLE_API_KEY carregada com sucesso.\")\n",
        "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "        logger.info(\"Cliente Google Generative AI configurado.\")\n",
        "    else:\n",
        "        logger.critical(\"Erro Crítico: A secret 'GOOGLE_API_KEY' ou variável de ambiente não foi encontrada.\")\n",
        "        raise RuntimeError(\"GOOGLE_API_KEY not found. A execução não pode continuar sem a API Key.\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Ocorreu um erro inesperado ao configurar a API Key ou o cliente GenAI: {e}\")\n",
        "    raise\n",
        "print(\"Bloco 3: Configuração da API Key e cliente GenAI processada.\")"
      ],
      "metadata": {
        "id": "uHOh02gbJ26q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 4: Configuração do Modelo Gemini e Função Auxiliar invocar_gemini\n",
        "try:\n",
        "    model_config = APP_CONFIG[\"gemini_model\"]\n",
        "    model = genai.GenerativeModel(\n",
        "        model_name=model_config[\"model_id\"],\n",
        "        generation_config=model_config[\"generation_config\"],\n",
        "        safety_settings=model_config[\"safety_settings\"]\n",
        "    )\n",
        "    logger.info(f\"Modelo Gemini '{model_config['model_id']}' inicializado com sucesso.\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Erro ao inicializar o modelo Gemini '{model_config['model_id']}': {e}\")\n",
        "    raise\n",
        "\n",
        "def invocar_gemini(prompt_usuario: str, contexto_sistema: str = \"Você é um assistente de pesquisa científica altamente qualificado.\") -> str:\n",
        "    full_prompt = (\n",
        "        f\"{contexto_sistema}\\n\\n\"\n",
        "        \"--- INÍCIO DA TAREFA DO USUÁRIO ---\\n\"\n",
        "        f\"{prompt_usuario}\\n\"\n",
        "        \"--- FIM DA TAREFA DO USUÁRIO ---\"\n",
        "    )\n",
        "    logger.info(f\"Invocando Gemini. Contexto: '{contexto_sistema[:50]}...', Prompt (primeiros 100 chars): {prompt_usuario[:100]}...\")\n",
        "\n",
        "    request_options = {\"timeout\": APP_CONFIG[\"gemini_model\"].get(\"request_timeout\", 60)} # Default 60s, configurado para 120s\n",
        "\n",
        "    try:\n",
        "        logger.debug(f\"Enviando para Gemini com timeout de {request_options['timeout']}s. Prompt completo (primeiros 200 chars): {full_prompt[:200]}\")\n",
        "        response = model.generate_content(full_prompt, request_options=request_options)\n",
        "        logger.info(\"Resposta recebida do Gemini.\")\n",
        "\n",
        "        if response.parts:\n",
        "            generated_text = \"\".join(part.text for part in response.parts if hasattr(part, 'text'))\n",
        "            logger.debug(f\"Texto gerado (das partes): {generated_text[:100]}...\")\n",
        "            return generated_text\n",
        "        elif hasattr(response, 'text') and response.text:\n",
        "             generated_text = response.text\n",
        "             logger.debug(f\"Texto gerado (do atributo text): {generated_text[:100]}...\")\n",
        "             return generated_text\n",
        "        else:\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                block_reason_message = getattr(response.prompt_feedback, 'block_reason_message', str(response.prompt_feedback.block_reason))\n",
        "                logger.warning(f\"Conteúdo bloqueado pelo Gemini. Razão: {block_reason_message}\")\n",
        "                return f\"Conteúdo bloqueado pela API. Razão: {block_reason_message}\"\n",
        "            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n",
        "                 generated_text = \"\".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))\n",
        "                 logger.debug(f\"Texto gerado (dos candidatos): {generated_text[:100]}...\")\n",
        "                 return generated_text\n",
        "            logger.warning(\"Resposta do Gemini vazia ou em formato inesperado. Feedback: %s\", response.prompt_feedback)\n",
        "            return \"Resposta vazia ou em formato inesperado do modelo. Verifique os logs e a configuração de segurança.\"\n",
        "    except Exception as e:\n",
        "        logger.exception(f\"Erro crítico ao interagir com o modelo Gemini: {e}\")\n",
        "        return f\"Erro crítico ao contatar o modelo Gemini: {str(e)}. Verifique os logs do servidor.\"\n",
        "\n",
        "print(\"Bloco 4: Modelo Gemini e função invocar_gemini (com timeout e logging) definidos.\")"
      ],
      "metadata": {
        "id": "iTH4PV5OJ7H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 5: Definição da Classe Base para Módulos (ModuloSynapseBase)\n",
        "class ModuloSynapseBase:\n",
        "    def __init__(self, nome_modulo: str, contexto_sistema_especifico: str, instrucao_principal: str):\n",
        "        self.nome_modulo = nome_modulo\n",
        "        self.contexto_sistema_completo = (\n",
        "            f\"Você é o {self.nome_modulo} do Synapse Intellect. \"\n",
        "            f\"{contexto_sistema_especifico} \"\n",
        "            \"Responda de forma estruturada, seguindo as diretrizes fornecidas. \"\n",
        "            \"A entrada do usuário estará claramente delimitada.\"\n",
        "        )\n",
        "        self.instrucao_principal = instrucao_principal\n",
        "\n",
        "    def construir_prompt_usuario_final(self, entrada_usuario: str) -> str:\n",
        "        return (\n",
        "            f\"{self.instrucao_principal}\\n\\n\"\n",
        "            f\"ENTRADA DO USUÁRIO PARA ANÁLISE:\\n'''\\n{entrada_usuario}\\n'''\\n\\n\"\n",
        "            \"POR FAVOR, PROCEDA COM A ANÁLISE CONFORME AS DIRETRIZES DO MÓDULO.\"\n",
        "        )\n",
        "\n",
        "    def executar(self, entrada_usuario: str) -> str:\n",
        "        if not entrada_usuario or not entrada_usuario.strip():\n",
        "            logger.warning(f\"Entrada vazia recebida para o {self.nome_modulo}.\")\n",
        "            return \"Por favor, forneça uma entrada válida para o módulo.\"\n",
        "        prompt_usuario_final = self.construir_prompt_usuario_final(entrada_usuario)\n",
        "        logger.info(f\"Executando {self.nome_modulo} com entrada (primeiros 100 chars): {entrada_usuario[:100]}...\")\n",
        "        return invocar_gemini(prompt_usuario_final, self.contexto_sistema_completo)\n",
        "\n",
        "print(\"Bloco 5: Classe Base ModuloSynapseBase definida.\")"
      ],
      "metadata": {
        "id": "dB0tr4hHJ-eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 6: Implementação dos Módulos Funcionais\n",
        "\n",
        "class ModuloInteligenciaEstrategica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Inteligência Estratégica\",\n",
        "            contexto_sistema_especifico=\"Sua especialidade é identificar tendências, lacunas e oportunidades em pesquisa.\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise o seguinte tema ou problema de pesquisa fornecido.\\nSua análise deve incluir:\\n\"\n",
        "                \"1. Tendências emergentes e consolidadas.\\n2. Lacunas de conhecimento.\\n\"\n",
        "                \"3. Possíveis impactos futuros.\\n4. Palavras-chave relevantes.\\nSeja prospectivo e estratégico.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_a_inteligencia_estrategica(e: str): return ModuloInteligenciaEstrategica().executar(e)\n",
        "\n",
        "class ModuloIdeacaoValidacao(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Ideação e Validação\",\n",
        "            contexto_sistema_especifico=\"Seu foco é gerar e refinar ideias de pesquisa, avaliando originalidade e conexões.\",\n",
        "            instrucao_principal=(\n",
        "                \"Trabalhe com a seguinte ideia ou problema de pesquisa:\\nSua tarefa é:\\n\"\n",
        "                \"1. Gerar 3-5 novas perguntas de pesquisa.\\n2. Avaliar originalidade.\\n\"\n",
        "                \"3. Sugerir 2-3 conexões interdisciplinares.\\n4. Apontar desafios.\\nFomente criatividade e rigor.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_b_ideacao_e_validacao(e: str): return ModuloIdeacaoValidacao().executar(e)\n",
        "\n",
        "class ModuloArquiteturaMetodologica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Arquitetura Metodológica\",\n",
        "            contexto_sistema_especifico=\"Sua especialidade é desenhar planos de pesquisa robustos.\",\n",
        "            instrucao_principal=(\n",
        "                \"Desenvolva um esboço de plano metodológico para:\\nSeu esboço deve considerar:\\n\"\n",
        "                \"1. Objetivos SMART.\\n2. Abordagem metodológica.\\n3. Amostragem/corpus.\\n\"\n",
        "                \"4. Fases da pesquisa.\\n5. Instrumentos/técnicas.\\nEstrutura clara e lógica.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_c_arquitetura_metodologica(e: str): return ModuloArquiteturaMetodologica().executar(e)\n",
        "\n",
        "class ModuloRevisaoCriticaEtica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Revisão Crítica & Ética\",\n",
        "            contexto_sistema_especifico=\"Atue como revisor experiente: rigor, viabilidade, ética.\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise a seguinte proposta/ideia/rascunho:\\nSua análise crítica deve abordar:\\n\"\n",
        "                \"1. Clareza do problema/objetivos.\\n2. Viabilidade.\\n3. Potencial de impacto.\\n\"\n",
        "                \"4. Possíveis vieses e mitigação.\\n5. Considerações éticas.\\nSeja construtivo e rigoroso.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_d_revisao_critica_e_etica(e: str): return ModuloRevisaoCriticaEtica().executar(e)\n",
        "\n",
        "class ModuloEscritaCientifica(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Escrita Científica\",\n",
        "            contexto_sistema_especifico=\"Ajude a aprimorar textos acadêmicos: clareza, estrutura, rigor.\",\n",
        "            instrucao_principal=(\n",
        "                \"Trabalhe com o seguinte texto/solicitação:\\nSua tarefa pode ser (adapte):\\n\"\n",
        "                \"1. Rascunho de seção: melhorias.\\n2. Ideia para parágrafo/seção: desenvolvimento.\\n\"\n",
        "                \"3. Dúvida sobre estilo/normas: orientações gerais.\\n4. Pedido de esboço: estrutura típica.\\nPriorize objetividade, precisão, concisão.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_e_escrita_cientifica(e: str): return ModuloEscritaCientifica().executar(e)\n",
        "\n",
        "class ModuloImpactoOperacoes(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Impacto & Operações Científicas\",\n",
        "            contexto_sistema_especifico=\"Oriente sobre estratégias de publicação, visibilidade e gestão.\",\n",
        "            instrucao_principal=(\n",
        "                \"Forneça orientações estratégicas para:\\nSuas orientações devem cobrir:\\n\"\n",
        "                \"1. Sugestões de periódicos/conferências.\\n2. Estratégias para visibilidade/impacto.\\n\"\n",
        "                \"3. KPIs para monitorar progresso.\\n4. Dicas para submissão.\\nConselhos práticos.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_f_impacto_e_operacoes_cientificas(e: str): return ModuloImpactoOperacoes().executar(e)\n",
        "\n",
        "class ModuloCientometriaAplicada(ModuloSynapseBase):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            nome_modulo=\"Módulo de Cientometria Aplicada\",\n",
        "            contexto_sistema_especifico=\"Explique e ajude a interpretar métricas científicas (conhecimento geral).\",\n",
        "            instrucao_principal=(\n",
        "                \"Analise a seguinte consulta/dados sobre métricas:\\nSua resposta deve:\\n\"\n",
        "                \"1. Pergunta geral: explique conceito, cálculo, uso, limitações.\\n\"\n",
        "                \"2. Autor/artigo/revista: métricas relevantes e interpretação.\\n\"\n",
        "                \"3. Dados numéricos: interpretação contextualizada.\\nSem acesso a bases de dados em tempo real.\"\n",
        "            )\n",
        "        )\n",
        "def modulo_g_cientometria_aplicada(e: str): return ModuloCientometriaAplicada().executar(e)\n",
        "\n",
        "print(\"Bloco 6: Módulos funcionais (todos usando Classe Base) definidos.\")"
      ],
      "metadata": {
        "id": "Lmp7jLRZKBJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 7: Mapeamento dos Módulos para a Interface Gradio\n",
        "modulos_disponiveis = {\n",
        "    \"A. Inteligência Estratégica\": modulo_a_inteligencia_estrategica,\n",
        "    \"B. Ideação e Validação\": modulo_b_ideacao_e_validacao,\n",
        "    \"C. Arquitetura Metodológica\": modulo_c_arquitetura_metodologica,\n",
        "    \"D. Revisão Crítica & Ética\": modulo_d_revisao_critica_e_etica,\n",
        "    \"E. Escrita Científica\": modulo_e_escrita_cientifica,\n",
        "    \"F. Impacto & Operações Científicas\": modulo_f_impacto_e_operacoes_cientificas,\n",
        "    \"G. Cientometria Aplicada\": modulo_g_cientometria_aplicada,\n",
        "}\n",
        "nomes_modulos_dropdown = list(modulos_disponiveis.keys())\n",
        "\n",
        "logger.info(\"Mapeamento de módulos para Gradio criado.\")\n",
        "print(\"Bloco 7: Mapeamento de módulos para Gradio criado.\")"
      ],
      "metadata": {
        "id": "EvIgWTcLKE03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 8: Esboço Conceitual para Geração Aumentada por Recuperação (RAG) - Comentado\n",
        "# Esta seção é um placeholder e requer implementação com seus próprios dados e configuração detalhada.\n",
        "# As bibliotecas LangChain e LlamaIndex foram instaladas no Bloco 1.\n",
        "\n",
        "# --- INÍCIO DO CÓDIGO RAG DE EXEMPLO (DESCOMENTE E ADAPTE PARA TESTAR) ---\n",
        "\n",
        "# --- Preparação de Dados de Exemplo (VOCÊ PRECISA CRIAR ESTES ARQUIVOS) ---\n",
        "# Exemplo: Crie 'doc_exemplo_1.txt' e 'doc_exemplo_2.txt' no seu ambiente.\n",
        "\n",
        "# rag_retriever = None\n",
        "# rag_chain = None\n",
        "\n",
        "# def configurar_rag_simples():\n",
        "#     global rag_retriever, rag_chain\n",
        "#     logger.info(\"Tentando configurar o RAG simples...\")\n",
        "#     try:\n",
        "#         # Verifique se os arquivos de exemplo existem\n",
        "#         if not (os.path.exists(\"doc_exemplo_1.txt\") and os.path.exists(\"doc_exemplo_2.txt\")):\n",
        "#             logger.warning(\"Arquivos de exemplo para RAG (doc_exemplo_1.txt, doc_exemplo_2.txt) não encontrados. RAG não será configurado.\")\n",
        "#             return\n",
        "\n",
        "#         from langchain_community.document_loaders import TextLoader\n",
        "#         from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#         from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "#         from langchain_community.vectorstores import FAISS\n",
        "#         from langchain.chains import RetrievalQA\n",
        "#         from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#         loader1 = TextLoader('doc_exemplo_1.txt', encoding='utf-8')\n",
        "#         loader2 = TextLoader('doc_exemplo_2.txt', encoding='utf-8')\n",
        "#         documents = loader1.load() + loader2.load()\n",
        "\n",
        "#         if not documents:\n",
        "#             logger.warning(\"Nenhum documento carregado para o RAG.\")\n",
        "#             return\n",
        "\n",
        "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "#         docs_split = text_splitter.split_documents(documents)\n",
        "\n",
        "#         if not docs_split:\n",
        "#             logger.warning(\"Nenhum documento dividido para o RAG.\")\n",
        "#             return\n",
        "\n",
        "#         embeddings = GoogleGenerativeAIEmbeddings(model=APP_CONFIG[\"rag\"][\"embeddings_model_name\"], task_type=\"retrieval_document\")\n",
        "#         vector_store = FAISS.from_documents(docs_split, embeddings)\n",
        "#         rag_retriever = vector_store.as_retriever(search_kwargs={\"k\": APP_CONFIG[\"rag\"][\"retriever_k\"]})\n",
        "\n",
        "#         llm_rag = ChatGoogleGenerativeAI(model=APP_CONFIG[\"gemini_model\"][\"model_id\"],\n",
        "#                                          temperature=0.5,\n",
        "#                                          convert_system_message_to_human=True)\n",
        "\n",
        "#         rag_chain = RetrievalQA.from_chain_type(\n",
        "#             llm=llm_rag,\n",
        "#             chain_type=\"stuff\",\n",
        "#             retriever=rag_retriever,\n",
        "#             return_source_documents=True\n",
        "#         )\n",
        "#         logger.info(\"Chain RetrievalQA (RAG) configurado com sucesso usando documentos de exemplo.\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.exception(f\"Erro ao configurar o exemplo RAG: {e}\")\n",
        "#         rag_chain = None # Garante que não será usado se falhar\n",
        "\n",
        "# def consultar_rag_com_gemini(query: str) -> str:\n",
        "#     if rag_chain is None:\n",
        "#         logger.warning(\"Sistema RAG (chain RetrievalQA) não configurado. Chamada ignorada.\")\n",
        "#         return \"Sistema RAG não está configurado. Verifique a célula de configuração RAG e os arquivos de exemplo.\"\n",
        "#     try:\n",
        "#         logger.info(f\"Consultando RAG com query: {query[:100]}...\")\n",
        "#         response = rag_chain.invoke({\"query\": query})\n",
        "#         answer = response.get(\"result\", \"Não foi possível obter uma resposta do chain RAG.\")\n",
        "#         # source_documents = response.get(\"source_documents\", [])\n",
        "#         # if source_documents:\n",
        "#         #     answer += \"\\n\\n--- Documentos Fonte Recuperados (RAG) ---\"\n",
        "#         #     for i, doc in enumerate(source_documents):\n",
        "#         #         answer += f\"\\nFonte {i+1}: {doc.page_content[:100]}...\"\n",
        "#         return answer\n",
        "#     except Exception as e:\n",
        "#         logger.exception(f\"Erro durante a consulta RAG: {e}\")\n",
        "#         return f\"Erro durante a consulta RAG: {str(e)}\"\n",
        "\n",
        "# Descomente a linha abaixo para tentar configurar o RAG ao iniciar (requer arquivos de exemplo)\n",
        "# configurar_rag_simples()\n",
        "\n",
        "# --- FIM DO CÓDIGO RAG DE EXEMPLO ---\n",
        "\n",
        "logger.info(\"Esboço conceitual RAG definido (atualmente comentado).\")\n",
        "print(\"Bloco 8: Esboço RAG (comentado) definido.\")"
      ],
      "metadata": {
        "id": "QCYyDAEOKHjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 9: Função da Interface Gradio (interface_synapse_intellect)\n",
        "def interface_synapse_intellect(reflexao_inicial, modulo_selecionado, texto_para_modulo, usar_rag_checkbox):\n",
        "    # Log inicial da requisição\n",
        "    log_prefix = \"Interface Gradio:\"\n",
        "    logger.info(f\"{log_prefix} Módulo: '{modulo_selecionado}', RAG: {usar_rag_checkbox}\")\n",
        "    logger.debug(f\"{log_prefix} Reflexão: {str(reflexao_inicial)[:100]}...\")\n",
        "    logger.debug(f\"{log_prefix} Entrada: {str(texto_para_modulo)[:100]}...\")\n",
        "\n",
        "    if not modulo_selecionado:\n",
        "        logger.warning(f\"{log_prefix} Nenhum módulo selecionado.\")\n",
        "        return \"Por favor, selecione um módulo.\", \"Nenhum módulo foi selecionado.\"\n",
        "    if not texto_para_modulo or not str(texto_para_modulo).strip():\n",
        "        logger.warning(f\"{log_prefix} Entrada vazia para '{modulo_selecionado}'.\")\n",
        "        return \"Por favor, forneça a entrada específica para o módulo.\", f\"A entrada para '{modulo_selecionado}' está vazia.\"\n",
        "\n",
        "    entrada_completa_para_modulo = str(texto_para_modulo)\n",
        "    if reflexao_inicial and str(reflexao_inicial).strip():\n",
        "        entrada_completa_para_modulo = (\n",
        "            f\"Contexto/Reflexão Inicial do Pesquisador:\\n'''\\n{str(reflexao_inicial)}\\n'''\\n\\n\"\n",
        "            f\"Tarefa Específica para o Módulo '{modulo_selecionado}':\\n'''\\n{str(texto_para_modulo)}\\n'''\"\n",
        "        )\n",
        "        logger.info(f\"{log_prefix} Reflexão inicial combinada com a entrada do módulo.\")\n",
        "\n",
        "    resultado_modulo = \"Processando...\" # Placeholder inicial\n",
        "    debrief_base_text = f\"Synapse Intellect (Módulo: {modulo_selecionado}) processou sua solicitação.\"\n",
        "\n",
        "    # A funcionalidade RAG permanece como esboço e não será ativada.\n",
        "    if usar_rag_checkbox:\n",
        "        logger.warning(f\"{log_prefix} RAG solicitado, mas não está ativo/implementado.\")\n",
        "        debrief_base_text += \" (RAG solicitado mas não ativo).\"\n",
        "        # Poderia adicionar um aviso ao resultado_modulo aqui se quisesse.\n",
        "\n",
        "    funcao_modulo = modulos_disponiveis.get(modulo_selecionado)\n",
        "    if funcao_modulo:\n",
        "        logger.info(f\"{log_prefix} Chamando função para o módulo '{modulo_selecionado}'.\")\n",
        "        resultado_modulo = funcao_modulo(entrada_completa_para_modulo) # Chamada direta\n",
        "        logger.info(f\"{log_prefix} Resultado recebido do módulo '{modulo_selecionado}'.\")\n",
        "        debrief_base_text += \" (Resposta gerada pelo modelo Gemini).\"\n",
        "    else:\n",
        "        logger.error(f\"{log_prefix} Módulo '{modulo_selecionado}' não encontrado no mapeamento.\")\n",
        "        resultado_modulo = f\"Erro: Módulo '{modulo_selecionado}' não encontrado.\"\n",
        "        debrief_base_text = \"Erro crítico: Módulo não implementado.\"\n",
        "\n",
        "    # Debrief gerado pelo LLM\n",
        "    prompt_debrief_llm = (\n",
        "        f\"O usuário interagiu com o '{modulo_selecionado}'. \"\n",
        "        f\"Descreva brevemente (1-2 sentenças) a estratégia ou foco principal deste módulo. Não resuma a resposta ao usuário.\"\n",
        "    )\n",
        "    contexto_debrief_llm = \"Você é um assistente IA que descreve a função de outros módulos IA de forma concisa.\"\n",
        "    debrief_gerado_pelo_llm = invocar_gemini(prompt_debrief_llm, contexto_debrief_llm)\n",
        "\n",
        "    final_debrief_text = f\"{debrief_base_text}\\n\\nEstratégia do Módulo (sugerida pela IA): {debrief_gerado_pelo_llm}\"\n",
        "    return resultado_modulo, final_debrief_text\n",
        "\n",
        "print(\"Bloco 9: Função da interface Gradio definida.\")"
      ],
      "metadata": {
        "id": "yTba7DsiKK10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 10: Criação da Interface Gradio (com CSS Customizado)\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body, .gradio-container {\n",
        "    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;\n",
        "    background-color: #f0f2f5; /* Um cinza muito claro, estilo Apple */\n",
        "    color: #1d1d1f; /* Texto principal escuro */\n",
        "}\n",
        ".gr-panel { /* Usado para agrupar colunas/componentes */\n",
        "    background-color: #ffffff !important;\n",
        "    border: 1px solid #d2d2d7 !important; /* Borda sutil */\n",
        "    border-radius: 12px !important;\n",
        "    box-shadow: 0 1px 3px rgba(0,0,0,0.04), 0 1px 2px rgba(0,0,0,0.08) !important; /* Sombra mais sutil */\n",
        "    padding: 24px !important;\n",
        "}\n",
        ".gr-button {\n",
        "    background-color: #007aff !important; /* Azul Apple */\n",
        "    color: white !important;\n",
        "    border-radius: 8px !important;\n",
        "    font-weight: 500 !important;\n",
        "    border: none !important;\n",
        "    padding: 10px 18px !important;\n",
        "    box-shadow: 0 1px 2px rgba(0,0,0,0.05) !important;\n",
        "}\n",
        ".gr-button:hover {\n",
        "    background-color: #0056b3 !important; /* Azul mais escuro no hover */\n",
        "}\n",
        ".gr-input input, .gr-textbox textarea, .gr-dropdown select, .gr-checkboxgroup input {\n",
        "    background-color: #f8f8fa !important; /* Fundo do campo de entrada um pouco mais claro */\n",
        "    border-radius: 8px !important;\n",
        "    border: 1px solid #c6c6c8 !important; /* Borda mais sutil */\n",
        "    color: #1d1d1f !important;\n",
        "    padding: 10px !important;\n",
        "}\n",
        ".gr-dropdown select { padding-right: 30px !important; } /* Espaço para a seta */\n",
        "\n",
        ".gr-markdown h1 { font-size: 28px; font-weight: 600; color: #1d1d1f; margin-bottom: 12px; }\n",
        ".gr-markdown h3 { font-size: 18px; font-weight: 600; color: #1d1d1f; margin-top: 20px; margin-bottom: 8px; }\n",
        ".gr-markdown p, .gr-markdown li { color: #333333; line-height: 1.6; }\n",
        "\n",
        "label span, .gr-checkbox label span { /* Texto dos labels */\n",
        "    color: #555 !important;\n",
        "    font-weight: 500 !important;\n",
        "    font-size: 14px !important;\n",
        "}\n",
        ".gr-info { /* Texto de informação abaixo dos componentes */\n",
        "    color: #6e6e73 !important;\n",
        "    font-size: 12px !important;\n",
        "}\n",
        "#resultado_modulo_md { /* Saída principal Markdown */\n",
        "    background-color: #f8f9fa;\n",
        "    padding: 18px;\n",
        "    border-radius: 8px;\n",
        "    border: 1px solid #e9ecef;\n",
        "    min-height: 250px; /* Altura mínima para a área de resultado */\n",
        "}\n",
        "#output_debrief_modulo textarea { /* Textbox do Debrief */\n",
        "    background-color: #e9ecef !important; /* Fundo levemente diferente para o debrief */\n",
        "    font-size: 13px !important;\n",
        "    line-height: 1.5 !important;\n",
        "}\n",
        "/* Remover borda padrão do Gradio container se o tema Soft adicionar uma */\n",
        ".gradio-container { border: none !important; box-shadow: none !important; }\n",
        "\"\"\"\n",
        "try:\n",
        "    with gr.Blocks(theme=gr.themes.Default(text_size=gr.themes.sizes.text_md), css=custom_css, title=\"Synapse Intellect™ v1.3\") as app_synapse: # Mudei para Default para ter mais controle com CSS\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # 🧠 Synapse Intellect™\n",
        "            ### v1.3 - Seu Copiloto de IA para Pesquisa Científica\n",
        "            \"\"\"\n",
        "        )\n",
        "        with gr.Row(variant=\"panel\"): # Usando variant=\"panel\" para aplicar o estilo .gr-panel\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Fase 0: Reflexão Inicial (Opcional)\")\n",
        "                input_reflexao = gr.Textbox(\n",
        "                    label=\"Descreva seu problema de pesquisa ou ideia geral aqui\",\n",
        "                    lines=3, # Reduzido para interface mais compacta\n",
        "                    placeholder=\"Ex: Dificuldades na adesão ao tratamento...\"\n",
        "                )\n",
        "                gr.Markdown(\"### ⚙️ Módulo e Entrada\")\n",
        "                input_modulo_selecionado = gr.Dropdown(\n",
        "                    choices=nomes_modulos_dropdown,\n",
        "                    label=\"Selecione o Módulo Funcional\",\n",
        "                    info=\"Escolha a função que deseja utilizar.\"\n",
        "                )\n",
        "                input_texto_para_modulo = gr.Textbox(\n",
        "                    label=\"Entrada Específica para o Módulo\",\n",
        "                    lines=6, # Reduzido\n",
        "                    placeholder=\"Forneça detalhes, perguntas, temas...\"\n",
        "                )\n",
        "                input_usar_rag = gr.Checkbox(\n",
        "                    label=\"Usar RAG (Funcionalidade em Esboço - NÃO ATIVA)\",\n",
        "                    value=False,\n",
        "                    interactive=False # Mantido desabilitado\n",
        "                )\n",
        "                botao_processar = gr.Button(\"🚀 Processar com Synapse Intellect\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### 💡 Resultados Gerados\")\n",
        "                output_resultado_modulo = gr.Markdown(elem_id=\"resultado_modulo_md\", label=\"Resultado do Módulo\")\n",
        "                output_debrief_modulo = gr.Textbox(\n",
        "                    elem_id=\"output_debrief_modulo\",\n",
        "                    label=\"Synapse Strategy Debrief™\",\n",
        "                    lines=6, # Reduzido\n",
        "                    interactive=False,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            ---\n",
        "            <p style=\"text-align:center; font-size:12px; color:#888;\">\n",
        "            Protótipo v1.3. Valide criticamente as respostas. RAG não ativo.\n",
        "            </p>\n",
        "            \"\"\"\n",
        "        )\n",
        "    logger.info(\"Interface Gradio v1.3 definida com CSS customizado.\")\n",
        "    print(\"Bloco 10: Interface Gradio v1.3 definida.\")\n",
        "except Exception as e:\n",
        "    logger.exception(\"Erro ao definir a interface Gradio v1.3.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "7etH8iTGKNfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bloco 11: Lançamento da Interface Gradio (Execução Principal)\n",
        "\n",
        "# Certifique-se que 'app_synapse' foi definido no Bloco 10\n",
        "if 'app_synapse' in locals():\n",
        "    logger.info(\"Iniciando a interface Gradio do Synapse Intellect™ v1.3...\")\n",
        "    # A linha abaixo é a que efetivamente inicia o Gradio em um notebook\n",
        "    app_synapse.launch(share=True, debug=True, inline=False)\n",
        "    logger.info(\"Interface Gradio iniciada. Verifique o link público acima.\")\n",
        "else:\n",
        "    logger.error(\"O objeto 'app_synapse' (interface Gradio) não foi definido. Verifique erros nos blocos anteriores, especialmente o Bloco 10.\")\n",
        "\n",
        "print(\"Bloco 11: Tentativa de lançamento da interface Gradio concluída.\")\n",
        "print(\"Se a interface iniciou, você verá um link '.gradio.live' na saída acima desta mensagem.\")\n",
        "print(\"Se não houver link ou houver erros, revise as saídas das células anteriores e os logs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fW--nmuKQVA",
        "outputId": "05d889e8-8fee-4b6c-c6ab-6420d0cf44cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://aec6e9436b66f065de.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Erro crítico ao interagir com o modelo Gemini: HTTPConnectionPool(host='localhost', port=41941): Read timed out. (read timeout=120.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-31-d33493dd05d3>\", line 27, in invocar_gemini\n",
            "    response = model.generate_content(full_prompt, request_options=request_options)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 868, in generate_content\n",
            "    # - It may require specifying regional endpoints when creating the service\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1336, in __call__\n",
            "    response, generative_service.GenerateContentResponse\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1236, in _get_response\n",
            "    Args:\n",
            "          \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 537, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=41941): Read timed out. (read timeout=120.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uLEM4N9KUgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}